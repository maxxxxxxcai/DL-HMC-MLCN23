{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 05:30:45,697 - Created a temporary directory at /tmp/tmplx4a3rn6\n",
      "2024-04-08 05:30:45,701 - Writing /tmp/tmplx4a3rn6/_remote_module_non_scriptable.py\n",
      "MONAI version: 1.0.1\n",
      "Numpy version: 1.23.4\n",
      "Pytorch version: 1.13.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8271a193229fe4437026185e218d5b06f7c8ce69\n",
      "MONAI __file__: /home1/zc348/anaconda3/envs/dl-hmc_2301c/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 4.0.2\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.2.0\n",
      "Tensorboard version: 2.11.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.14.1\n",
      "tqdm version: 4.64.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.4.3\n",
      "einops version: 0.6.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import monai\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadImaged, AddChanneld, Orientationd, \\\n",
    "    Spacingd, \\\n",
    "    ToTensord,  \\\n",
    "    DataStatsd, \\\n",
    "    ToDeviced\n",
    "from monai.data import list_data_collate\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchsummary import summary\n",
    "monai.config.print_config()\n",
    "import sys\n",
    "# sys.path.append(r'/data16/private/zc348/project/DL_HMC_attention/mCT/util/python')\n",
    "sys.path.append(r'/data16/private/zc348/project/DL_HMC_attention/util/python')\n",
    "import vicra_toolbox\n",
    "import nibabel; nibabel.imageglobals.logger.setLevel(40)\n",
    "# New transforms \n",
    "\n",
    "sys.path.append(r'../')\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "from dlhmc.transforms import (\n",
    "    CreateImageStack,\n",
    "    ComputeRelativeMotion,\n",
    "    RandSamplePET,\n",
    "    ComputeRelativeMotiond,\n",
    "    CreateImageStackd,\n",
    "    RandSamplePETd,\n",
    ")\n",
    "\n",
    "from dlhmc.utils.data import (\n",
    "    concatenate_vicra,\n",
    "    split_dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_set_all=['AH017', 'AL819', 'AL831', 'AO822', 'AT653', 'AT687', 'BB302',\n",
    "       'BD233', 'BJ464', 'BK384', 'MM484', 'BR043', 'BW083', 'CA557',\n",
    "       'CB411', 'CD348', 'CF952', 'CH144', 'CK217', 'CM087', 'CM712',\n",
    "       'DB031', 'DB371', 'DB781', 'DF607', 'DG649', 'DH285', 'DM113',\n",
    "       'DM524', 'DS880', 'EC569', 'EC599', 'EG252', 'EK470', 'ER338',\n",
    "       'ET172', 'EW491', 'FF840', 'GB696', 'GL357', 'HW920', 'JB080',\n",
    "       'JB472', 'JB737', 'JD861', 'JF625', 'JH281', 'JR857', 'JS053',\n",
    "       'JS270', 'JS315', 'KA848', 'KC328', 'KN938', 'KR673', 'KR673',\n",
    "       'KS017', 'KS017', 'KS185', 'LC157', 'LJ271', 'LL150', 'LL783',\n",
    "       'LS439', 'MC549', 'MD602', 'MF885', 'MJ068', 'MJ291', 'MK283',\n",
    "       'MM520', 'MR627', 'NC298', 'NF861', 'NJ610', 'NP335', 'PC682',\n",
    "       'RC963', 'RH479', 'RK887', 'RM802', 'RS467', 'RT805', 'SA836',\n",
    "       'SE511', 'SH996', 'SK238', 'SL436', 'SL855', 'SM529', 'SR015',\n",
    "       'TB254', 'TB937', 'TF645', 'TH497', 'TK231', 'UB142', 'WB408',\n",
    "       'WL971', 'WM493']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i in tr_set_all for i in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=['PF605', 'JB538', 'AF120', 'JG369', 'SY636',\n",
    "'EC950', 'NM937', 'AS469', 'CH568', 'JO308',\n",
    "'CJ509', 'SY869', 'BB688', 'HR322', 'DS636',\n",
    " 'JR684', 'JM100', 'SM968', 'TM628', 'MC181']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n",
      "Dataset contains total 1,800 entries\n",
      "Dataset split into 3 subsets:\n",
      " Train set size: 288\n",
      " Val set size: 36\n",
      " Test set size: 36\n"
     ]
    }
   ],
   "source": [
    "tr_set_all=['AH017', 'AL819', 'AL831', 'AO822', 'AT653', 'AT687', 'BB302',\n",
    "       'BD233', 'BJ464', 'BK384', 'MM484', 'BR043', 'BW083', 'CA557',\n",
    "       'CB411', 'CD348', 'CF952', 'CH144', 'CK217', 'CM087', 'CM712',\n",
    "       'DB031', 'DB371', 'DB781', 'DF607', 'DG649', 'DH285', 'DM113',\n",
    "       'DM524', 'DS880', 'EC569', 'EC599', 'EG252', 'EK470', 'ER338',\n",
    "       'ET172', 'EW491', 'FF840', 'GB696', 'GL357', 'HW920', 'JB080',\n",
    "       'JB472', 'JB737', 'JD861', 'JF625', 'JH281', 'JR857', 'JS053',\n",
    "       'JS270', 'JS315', 'KA848', 'KC328', 'KN938', 'KR673', 'KR673',\n",
    "       'KS017', 'KS017', 'KS185', 'LC157', 'LJ271', 'LL150', 'LL783',\n",
    "       'LS439', 'MC549', 'MD602', 'MF885', 'MJ068', 'MJ291', 'MK283',\n",
    "       'MM520', 'MR627', 'NC298', 'NF861', 'NJ610', 'NP335', 'PC682',\n",
    "       'RC963', 'RH479', 'RK887', 'RM802', 'RS467', 'RT805', 'SA836',\n",
    "       'SE511', 'SH996', 'SK238', 'SL436', 'SL855', 'SM529', 'SR015',\n",
    "       'TB254', 'TB937', 'TF645', 'TH497', 'TK231', 'UB142', 'WB408',\n",
    "       'WL971', 'WM493']\n",
    "\n",
    "test_set=['PF605', 'JB538', 'AF120', 'JG369', 'SY636',\n",
    "'EC950', 'NM937', 'AS469', 'CH568', 'JO308',\n",
    "'CJ509', 'SY869', 'BB688', 'HR322', 'DS636',\n",
    " 'JR684', 'JM100', 'SM968', 'TM628', 'MC181']\n",
    "# test_set=['AO101', 'PF605', 'JO308', 'AS469', 'JR684']\n",
    "tr_set = tr_set_all[0:100]\n",
    "from dataset_summary_toolbox import compute_delta_T \n",
    "from data_prep_toolbox import delta_T_magnitude, Relative_motion_A_to_B_12, build_legal_dataset, deal_dataframe, clean_df\n",
    "from sampling_toolbox import data_split_sample, add_T_deltaT\n",
    "\n",
    "# Get the summaries\n",
    "\n",
    "summaries, delta_T_all=compute_delta_T(['FDG'], tr_set, data_type = 'real')\n",
    "\n",
    "def calculate_beta(df):\n",
    "    nums = df['T'].to_numpy()\n",
    "    trans, rot = 0, 0\n",
    "    for array in range(len(nums)):\n",
    "        for i in range(3):\n",
    "            trans += abs(nums[array][i])\n",
    "        for j in range(3):\n",
    "            rot += abs(nums[array][3 + j])\n",
    "#     print(\"trans:\", trans, \"rot:\", rot)\n",
    "    beta = trans/(trans + rot)\n",
    "#     print('beta:', beta)\n",
    "    return beta\n",
    "\n",
    "num_subsample = 360 #1800 # Number of samples per subject (to be split between training, validation and testing sets)\n",
    "\n",
    "train_list=list()\n",
    "val_list=list()\n",
    "test_list=list()\n",
    "\n",
    "# Sample training, validation and testing sets and save them in lists\n",
    "for summary in summaries: \n",
    "    summary['VICRA']=concatenate_vicra(summary)\n",
    "\n",
    "    print('Dataset contains total {:,d} entries'.format(len(summary)))\n",
    "    df_train, df_val, df_test = split_dataset(summary, num_subsample=num_subsample, test_size_percent=0.1, validation_size_percent=0.1)\n",
    "    \n",
    "    # train_beta_array = df_train.groupby('PatientID').apply(calculate_beta)\n",
    "    # train_beta_array = pd.DataFrame({'PatientID':train_beta_array.index, 'beta':train_beta_array.values})\n",
    "    # df_train = pd.merge(df_train, train_beta_array, on=\"PatientID\")\n",
    "\n",
    "    # val_beta_array = df_val.groupby('PatientID').apply(calculate_beta)\n",
    "    # val_beta_array = pd.DataFrame({'PatientID':val_beta_array.index, 'beta':val_beta_array.values})\n",
    "    # df_val = pd.merge(df_val, val_beta_array, on=\"PatientID\")\n",
    "\n",
    "    # test_beta_array = df_test.groupby('PatientID').apply(calculate_beta)\n",
    "    # test_beta_array = pd.DataFrame({'PatientID':test_beta_array.index, 'beta':test_beta_array.values})\n",
    "    # df_test = pd.merge(df_test, test_beta_array, on=\"PatientID\")\n",
    "    print('Dataset split into 3 subsets:')\n",
    "    print(' Train set size: {:,d}'.format(len(df_train)))\n",
    "    print(' Val set size: {:,d}'.format(len(df_val)))\n",
    "    print(' Test set size: {:,d}'.format(len(df_test)))\n",
    "    \n",
    "    train_dict={\n",
    "        'DataFrame':df_train\n",
    "    }\n",
    "\n",
    "    val_dict={\n",
    "        'DataFrame':df_val\n",
    "    }\n",
    "    \n",
    "    test_dict={\n",
    "        'DataFrame':df_test\n",
    "    }\n",
    "    \n",
    "    train_list.append(train_dict)\n",
    "    val_list.append(val_dict)\n",
    "    test_list.append(test_dict)\n",
    "\n",
    "\n",
    "    \n",
    "# SIZE = (64,64,64)\n",
    "SIZE = (32,32,32)\n",
    "\n",
    "image_key = \"ThreeD_Cloud\"\n",
    "meta_keys=['ScanStart','VICRA']\n",
    "\n",
    "# Need to format as a dictionary for input\n",
    "# sample_dict = {\n",
    "#     'DataFrame': df_train,\n",
    "    # 'PatientID': df_sample['PatientID'].unique()\n",
    "# }\n",
    "\n",
    "# Here, we only have one subject of data, so a small example, but more subjects can be added\n",
    "# to this list.\n",
    "# sample_data_dict = [sample_dict]\n",
    "\n",
    "pet_transforms = monai.transforms.Compose([\n",
    "    CreateImageStackd(keys=\"DataFrame\", image_key=image_key, spatial_size=SIZE),\n",
    "    RandSamplePETd(keys=image_key, meta_data_key=\"DataFrame\", meta_keys=meta_keys, num_samples=8), \n",
    "    ComputeRelativeMotiond(keys=['VICRA_ref','VICRA_mov'], output_key='VICRA_rel'),\n",
    "    monai.transforms.DeleteItemsd(keys=[\"DataFrame\", \"ScanStart\", \"VICRA_ref\", \"VICRA_mov\", image_key]),\n",
    "    monai.transforms.ToTensord(\n",
    "        keys=[\"ScanStart_ref\", \"ScanStart_mov\", \"VICRA_rel\"]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_ds = monai.data.PersistentDataset(\n",
    "    data=train_list, \n",
    "    transform=pet_transforms, \n",
    "    cache_dir=\"/data16/private/zc348/project/dev/100hrrt/cache/\"\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=12, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=monai.data.list_data_collate\n",
    ")\n",
    "\n",
    "val_ds = monai.data.PersistentDataset(\n",
    "    data=val_list, \n",
    "    transform=pet_transforms, \n",
    "    cache_dir=\"/data16/private/zc348/project/dev/100hrrt/cache/\"\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=12, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=monai.data.list_data_collate\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(UNet, self).__init__()\n",
    "        #Encode\n",
    "        self.conv_encode1 = self.contracting_block(in_channels=in_channel, out_channels=32)\n",
    "        self.conv_maxpool1 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "        self.conv_encode2 = self.contracting_block(32, 64)\n",
    "        self.conv_maxpool2 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "        self.conv_encode3 = self.contracting_block(64, 128)\n",
    "        self.conv_maxpool3 = torch.nn.MaxPool3d(kernel_size=2)\n",
    "\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        \"\"\"\n",
    "        This function creates one contracting block\n",
    "        \"\"\"\n",
    "        block = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(kernel_size=kernel_size, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            torch.nn.BatchNorm3d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv3d(kernel_size=kernel_size, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            torch.nn.BatchNorm3d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encode_block1 = self.conv_encode1(x)\n",
    "        encode_pool1 = self.conv_maxpool1(encode_block1)\n",
    "        encode_block2 = self.conv_encode2(encode_pool1)\n",
    "        encode_pool2 = self.conv_maxpool2(encode_block2)\n",
    "        \n",
    "        encode_block3 = self.conv_encode3(encode_pool2)\n",
    "        encode_pool3 = self.conv_maxpool3(encode_block3)\n",
    "\n",
    "\n",
    "        return encode_pool3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_attention(nn.Module):\n",
    "    def  __init__(self, all_channel=128, all_dim=4*4*4):\t\n",
    "\n",
    "        super(Cross_attention, self).__init__()\n",
    "        self.linear_e = nn.Linear(all_channel, all_channel,bias = False)\n",
    "        self.channel = all_channel\n",
    "        self.dim = all_dim\n",
    "        self.gate = nn.Conv3d(all_channel, 1, kernel_size  = 1, bias = False)\n",
    "        self.gate_s = nn.Sigmoid()\n",
    "        self.conv1 = nn.Conv3d(all_channel*1, all_channel, kernel_size=3, padding=1, bias = False)\n",
    "        self.conv2 = nn.Conv3d(all_channel*1, all_channel, kernel_size=3, padding=1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm3d(all_channel)\n",
    "        self.bn2 = nn.BatchNorm3d(all_channel)\n",
    "        self.prelu = nn.ReLU(inplace=True)\n",
    "        self.conv3d_7 = nn.Conv3d(in_channels=all_channel *2 , out_channels=all_channel, kernel_size=1, stride=(1, 1, 1), padding=0)\n",
    "        self.pathC_bn1 = nn.BatchNorm3d(all_channel*2)\n",
    "        self.conv3d_8 = nn.Conv3d(in_channels=all_channel, out_channels=all_channel//2, kernel_size=3, stride=(1, 1, 1), padding=1)\n",
    "        self.conv3d_9 = nn.Conv3d(in_channels=all_channel//2, out_channels=all_channel//8, kernel_size=3, stride=(1, 1, 1), padding=1)\n",
    "        self.pathC_bn2 = nn.BatchNorm3d(all_channel//8)\n",
    "\n",
    "        self.conva = nn.Conv3d(all_channel, all_channel, kernel_size=1, padding=0, bias = False)\n",
    "        self.convb = nn.Conv3d(all_channel, all_channel, kernel_size=1, padding=0, bias = False)\n",
    "    \n",
    "    \n",
    "\t\t\n",
    "    def forward(self, exemplar, query): \n",
    "        \n",
    "\t \n",
    "        fea_size = query.size()[2:]\t \n",
    "        \n",
    "        #### correlation matrix computation\n",
    "        exemplar_v =  self.convb(exemplar) \n",
    "        query_v =  self.convb(query) \n",
    "        exemplar_q =  self.conva(exemplar) \n",
    "        query_k =  self.conva(query) \n",
    "        exemplar_flat = exemplar_q.view(-1, self.channel, self.dim) #N,C,H*W\n",
    "        query_flat = query_k.view(-1, self.channel, self.dim)\n",
    "        exemplar_t = torch.transpose(exemplar_flat,1,2).contiguous()  #batch size x dim x num\n",
    "        A = torch.bmm(exemplar_t, query_flat)\n",
    "        A = F.softmax(A, dim = 1) \n",
    "        B = F.softmax(torch.transpose(A,1,2),dim=1)\n",
    "        query_att = torch.bmm(exemplar_v.view(-1, self.channel, self.dim) , A).contiguous() \n",
    "        exemplar_att = torch.bmm(query_v.view(-1, self.channel, self.dim), B).contiguous()\n",
    "        #####self-gate mechanism\n",
    "        input1_att = exemplar_att.view(-1, self.channel, fea_size[0], fea_size[1], fea_size[2])  \n",
    "        input2_att = query_att.view(-1, self.channel, fea_size[0], fea_size[1], fea_size[2])\n",
    "        input1_mask = self.gate(input1_att)\n",
    "        input2_mask = self.gate(input2_att)\n",
    "        input1_mask = self.gate_s(input1_mask)\n",
    "        input2_mask = self.gate_s(input2_mask)\n",
    "        input1_att = input1_att * input1_mask\n",
    "        input2_att = input2_att * input2_mask\n",
    "        input1_att = input1_att +  exemplar_v\n",
    "        input2_att = input2_att + query_v\n",
    "        ######Deep norm and fusion\n",
    "        input1_att  = self.conv1(input1_att )\n",
    "        input2_att  = self.conv2(input2_att ) \n",
    "        input1_att  = self.bn1(input1_att )\n",
    "        input2_att  = self.bn2(input2_att )\n",
    "        input1_att  = self.prelu(input1_att )\n",
    "        input2_att  = self.prelu(input2_att )\n",
    "        conv_input = torch.cat((exemplar, query), 1)\n",
    "\n",
    "        # conv_input = torch.cat((input1_att, input2_att), 1)\n",
    "        x = self.pathC_bn1(conv_input)\n",
    "        x = self.conv3d_7(x)\n",
    "        x = self.prelu(x)\n",
    "        x = self.conv3d_8(x)\n",
    "        x = self.prelu(x)\n",
    "        x = self.conv3d_9(x)\n",
    "        x = self.pathC_bn2(x)\n",
    "\n",
    "        return x.view(x.size()[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlhmc.utils import Relative_motion_A_to_B_12,RotTransMatrix_6Params\n",
    "\n",
    "def matrix_transformation(gt_reg):\n",
    "    i_6_list = list()\n",
    "    for i_12 in gt_reg:\n",
    "        i_6 = np.reshape(i_12,12)\n",
    "        i_6 = torch.from_numpy(RotTransMatrix_6Params(i_6,1))\n",
    "        i_6_list.append(i_6)\n",
    "        \n",
    "    gt_reg_6 = torch.stack(i_6_list).float()\n",
    "    # return gt_reg_6\n",
    "    return gt_reg_6.to(gt_reg.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PETRegNet_dataloader(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, dropout=0.3,img_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "             \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.feature_extractor = UNet(1,1)\n",
    "\n",
    "        self.coattention = Cross_attention()\n",
    "\n",
    "\n",
    "        self.regression_layers = torch.nn.Sequential(\n",
    "\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.Linear(128, 16),\n",
    "            torch.nn.Linear(16, 6),\n",
    "        )\n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        y1 = self.feature_extractor(x1)\n",
    "        y2 = self.feature_extractor(x2)\n",
    "        y = self.coattention(y1,y2)\n",
    "        y = self.regression_layers(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # set deterministic training for reproducibility\n",
    "        monai.utils.misc.set_determinism(seed=42)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x1 = batch[\"ThreeD_Cloud_ref\"]\n",
    "        x2 = batch[\"ThreeD_Cloud_mov\"]\n",
    "        ref_time = batch['ScanStart_ref']\n",
    "        mov_time = batch['ScanStart_mov']\n",
    "        # x_t = torch.stack([ref_time, mov_time], dim=1)\n",
    "        gt_reg = batch[\"VICRA_rel\"].float()\n",
    "        y = self.forward(x1,x2)\n",
    "       \n",
    "        target_six = matrix_transformation(gt_reg)\n",
    "        loss = self.loss_function(y, target_six)   \n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Calculate the average loss\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        # Logging at the end of every epoch\n",
    "        self.logger.experiment.add_scalar('Train/Loss', avg_loss, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x1 = batch[\"ThreeD_Cloud_ref\"]\n",
    "        x2 = batch[\"ThreeD_Cloud_mov\"]\n",
    "        ref_time = batch['ScanStart_ref']\n",
    "        mov_time = batch['ScanStart_mov']\n",
    "        x_t = torch.stack([ref_time, mov_time], dim=1)\n",
    "        gt_reg = batch[\"VICRA_rel\"].float()\n",
    "        y = self.forward(x1,x2)\n",
    "        target_six = matrix_transformation(gt_reg)\n",
    "        loss = self.loss_function(y, target_six)   \n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate the average loss\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        # Logging at the end of every epoch\n",
    "        self.logger.experiment.add_scalar('Val/Loss', avg_loss, self.current_epoch)\n",
    "\n",
    "        # Log the value for model checkpoint saving\n",
    "        self.log('val_loss', avg_loss.item()) #added .item(), otherwise validation wouldn't work \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        total_params = list(self.feature_extractor.parameters()) + list(self.regression_layers.parameters()) #+ list(self.coattention.parameters())# + list(self.conv_transfer.parameters()) #\n",
    "        opt = torch.optim.Adam(total_params, lr=5e-4)\n",
    "        scheduler = {'scheduler': torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=200, gamma=0.98),\n",
    "                     'name': 'Learning Rate'}\n",
    "        return [opt], [scheduler]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH=./saved_model_100hrrt_att_32_timeembeding(test)\n",
      "2024-02-20 13:34:39,622 - GPU available: True (cuda), used: True\n",
      "2024-02-20 13:34:39,623 - TPU available: False, using: 0 TPU cores\n",
      "2024-02-20 13:34:39,624 - IPU available: False, using: 0 IPUs\n",
      "2024-02-20 13:34:39,626 - HPU available: False, using: 0 HPUs\n",
      "2024-02-20 13:34:39,630 - Missing logger folder: ./saved_model_100hrrt_att_32_timeembeding(test)/logs/lightning_logs\n",
      "2024-02-20 13:34:39,634 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "2024-02-20 13:34:39,645 - \n",
      "  | Name              | Type            | Params\n",
      "------------------------------------------------------\n",
      "0 | feature_extractor | UNet            | 859 K \n",
      "1 | coattention       | Cross_attention | 1.2 M \n",
      "2 | regression_layers | Sequential      | 133 K \n",
      "3 | loss_function     | MSELoss         | 0     \n",
      "------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.838     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba81ca11723f4151a33c920f9771a15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6b3d6587dd4b39873484968cf9ea16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# from vio_attention_loader import PETRegNet_dataloader,PETRegNet_dataloader_dual\n",
    "model = PETRegNet_dataloader().to(device)\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "MODEL_PATH = os.path.join('.','saved_model_100hrrt_att_32_timeembeding(test)')\n",
    "print('MODEL_PATH={}'.format(MODEL_PATH))\n",
    "\n",
    "# Initialise the LightningModule\n",
    "# model = PETRegNet_dataloader(dropout=0.3)\n",
    "# model(dropout=0.3)\n",
    "# Set up loggers and checkpoints\n",
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=os.path.join(MODEL_PATH,'logs')\n",
    ")\n",
    "\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=os.path.join(MODEL_PATH),\n",
    "    filename=\"PETRegNet-{epoch}-{val_loss:.3f}\",\n",
    "    monitor='val_loss',\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Check for last checkpoint\n",
    "resume_checkpoint = None\n",
    "if os.path.exists(os.path.join(MODEL_PATH,'last.ckpt')):\n",
    "    resume_checkpoint = os.path.join(MODEL_PATH,'last.ckpt')\n",
    "\n",
    "# Initialise Lightning's trainer.\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=[1],\n",
    "    max_epochs=3000, #10k is generally enough for single-subjct studies\n",
    "    logger=tb_logger,\n",
    "    callbacks=[lr_monitor,checkpoint_callback],\n",
    "    num_sanity_val_steps=1,\n",
    "    check_val_every_n_epoch=20,\n",
    "#     resume_from_checkpoint=resume_checkpoint # deprecated since v1.5\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, train_loader, val_loader, ckpt_path=resume_checkpoint) # previously was trainer.fit(model, train_dataloader=tr_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing on testing patients (for multisubject studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_toolbox import build_df_results, show_df_loss, build_df_results_12, plot_vicra_network, plot_diff_vicra_network, save_synthetic_vicra, print_loss,plot_networks_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=['PF605', 'JB538', 'AF120', 'JG369', 'SY636',\n",
    "'EC950', 'NM937', 'AS469', 'CH568', 'JO308',\n",
    "'CJ509', 'SY869', 'BB688', 'HR322', 'DS636',\n",
    " 'JR684', 'JM100', 'SM968', 'TM628', 'MC181']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_summary_toolbox import compute_delta_T \n",
    "from data_prep_toolbox import delta_T_magnitude, Relative_motion_A_to_B_12, build_legal_dataset, deal_dataframe, clean_df\n",
    "from sampling_toolbox import data_split_sample, add_T_deltaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_test, delta_T_all_test=compute_delta_T(['FDG'], test_set,data_type='real')\n",
    "df_test=[]\n",
    "predictions=[]\n",
    "df_input_diff_all=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    df_test.append(deal_dataframe(summaries_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n"
     ]
    }
   ],
   "source": [
    "KEYS = ['ThreeD_Cloud_ref','ThreeD_Cloud_mov'] \n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=KEYS, reader='NibabelReader', as_closest_canonical=False),\n",
    "    AddChanneld(keys=KEYS), \n",
    "    Orientationd(keys=KEYS, axcodes='RAS'),\n",
    "    ToTensord(keys=KEYS)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PETRegNet_dataloader(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.feature_extractor = monai.networks.nets.resnet10(\n",
    "            spatial_dims=3, \n",
    "            n_input_channels=1, \n",
    "            num_classes=128\n",
    "        )\n",
    "\n",
    "        # self.fwt_layers = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(2, 32),\n",
    "        #     torch.nn.LeakyReLU(),\n",
    "        #     torch.nn.Dropout(dropout),\n",
    "        #     torch.nn.Linear(32, 64),\n",
    "        #     torch.nn.LeakyReLU(),\n",
    "        #     torch.nn.Dropout(dropout),\n",
    "        #     torch.nn.Linear(64, 128*2),            \n",
    "        # )\n",
    "\n",
    "        self.regression_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(256, 128),\n",
    "            # torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(128, 16),\n",
    "            # torch.nn.LeakyReLU(),\n",
    "            # torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(16, 6),\n",
    "        )\n",
    "        \n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "        self.l1 = torch.nn.L1Loss(size_average=None, reduce=None, reduction = 'mean')\n",
    "        self.sup_loss = []\n",
    "        self.cod_loss = []\n",
    "        self.epoch_num = 0\n",
    "        \n",
    "    def forward(self, x1, x2, x_t):\n",
    "        y1 = self.feature_extractor(x1)\n",
    "        y2 = self.feature_extractor(x2)\n",
    "        y = torch.cat([y1,y2], dim=-1)\n",
    "\n",
    "        # x_t=x_t.float()\n",
    "        # x_fwt = self.fwt_layers(torch.transpose(x_t[None],0,1))\n",
    "        # x_fwt=torch.squeeze(x_fwt)\n",
    "        # y = torch.mul(y, x_fwt)\n",
    "        y = self.regression_layers(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PETRegNet_dataloader(\n",
       "  (feature_extractor): ResNet(\n",
       "    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (fwt_layers): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=256, bias=True)\n",
       "  )\n",
       "  (regression_layers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss_function): MSELoss()\n",
       "  (l1): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from network import PETRegNet_dataloader\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "saved_model_path = '/data16/private/zc348/project/DL_HMC_attention/saved_model_100hrrt_dlhmc/PETRegNet-epoch=11909-val_loss=0.567.ckpt'\n",
    "loaded_model = PETRegNet_dataloader().load_from_checkpoint(saved_model_path)\n",
    "loaded_model.eval()\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PETRegNet_dataloader(\n",
       "  (feature_extractor): ResNet(\n",
       "    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3), bias=False)\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): ResNetBlock(\n",
       "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2))\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (regression_layers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss_function): MSELoss()\n",
       "  (l1): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "saved_model_path = '/data16/private/zc348/project/DL_HMC_attention/saved_model_100hrrt_att_32_stepsize200_resnetonly/PETRegNet-epoch=3689-val_loss=0.391.ckpt'\n",
    "loaded_model = PETRegNet_dataloader().load_from_checkpoint(saved_model_path)\n",
    "loaded_model.eval()\n",
    "loaded_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PETRegNet_dataloader(\n",
       "  (feature_extractor): UNet(\n",
       "    (conv_encode1): Sequential(\n",
       "      (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv_maxpool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv_encode2): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv_maxpool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv_encode3): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (conv_maxpool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (coattention): Cross_attention(\n",
       "    (linear_e): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (gate): Conv3d(128, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (gate_s): Sigmoid()\n",
       "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (prelu): ReLU(inplace=True)\n",
       "    (conv3d_7): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (pathC_bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3d_8): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (conv3d_9): Conv3d(64, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (pathC_bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conva): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    (convb): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "  )\n",
       "  (regression_layers): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss_function): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "saved_model_path = '/data16/private/zc348/project/DL_HMC_attention/saved_model_100hrrt_att_32_subnumuber360_stepsize200/save/PETRegNet-epoch=1619-val_loss=0.426.ckpt'\n",
    "loaded_model = PETRegNet_dataloader().load_from_checkpoint(saved_model_path)\n",
    "loaded_model.eval()\n",
    "loaded_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "df_results_all=[]\n",
    "y_list_all=[]\n",
    "df_input_diff_all=[]\n",
    "prediction_list = list()\n",
    "times = torch.zeros((1800*20,1)) \n",
    "# torch.cuda.synchronize()\n",
    "# starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "idx=0\n",
    "\n",
    "for j in range(len(test_set)):\n",
    "\n",
    "    df = df_test[j]\n",
    "\n",
    "    #fixed reference time\n",
    "    df_input_diff = vicra_toolbox.build_netinput_fixed_reference(df).reset_index()\n",
    "    pairs=[]\n",
    "    for i in range(len(df_input_diff)):\n",
    "        pairs.append(np.array([df_input_diff['ScanStart_ref'][i], df_input_diff['ScanStart_mov'][i]]))\n",
    "    df_input_diff['pairs']=pairs\n",
    "    \n",
    "    df_input_diff_all.append(df_input_diff)\n",
    "\n",
    "    ##building testing dataloader\n",
    "    test_dict = df_input_diff.to_dict('records')\n",
    "\n",
    "    for i in range(len(test_dict)):\n",
    "        x = test_dict[i]['ThreeD_Cloud_ref'].find('nii')\n",
    "        fn_cloud1 = test_dict[i]['ThreeD_Cloud_ref'][0:x] + 'nii_monai_resize'\n",
    "        x = x+3\n",
    "        y = test_dict[i]['ThreeD_Cloud_ref'].find('3dcld')\n",
    "        fn_cloud2 =  test_dict[i]['ThreeD_Cloud_ref'][x:y] + '3dcld_monai_rz.nii'\n",
    "        test_dict[i]['ThreeD_Cloud_ref'] = fn_cloud1 + fn_cloud2\n",
    "\n",
    "        x = test_dict[i]['ThreeD_Cloud_mov'].find('nii')\n",
    "        fn_cloud1 = test_dict[i]['ThreeD_Cloud_mov'][0:x] + 'nii_monai_resize'\n",
    "        x = x+3\n",
    "        y = test_dict[i]['ThreeD_Cloud_mov'].find('3dcld')\n",
    "        fn_cloud2 =  test_dict[i]['ThreeD_Cloud_mov'][x:y] + '3dcld_monai_rz.nii'\n",
    "        test_dict[i]['ThreeD_Cloud_mov'] = fn_cloud1 + fn_cloud2\n",
    "\n",
    "    # Create the Dataset\n",
    "#     ds_test = monai.data.CacheDataset(data=test_dict, transform=train_transforms)\n",
    "    ds_test = monai.data.Dataset(data=test_dict, transform=train_transforms)\n",
    "    #ds_tr = monai.data.SmartCacheDataset(data=tr_dict,transform=train_transforms,replace_rate=1,cache_num=64,shuffle=True)\n",
    "    # Create the DataLoader\n",
    "    test_loader = monai.data.DataLoader(ds_test, batch_size=32, num_workers=2, collate_fn=list_data_collate)\n",
    "    \n",
    "    #calculate loss function and network output\n",
    "    # saved_model_path = os.path.join(MODEL_PATH,'PETRegNet-epoch=3130-val_loss=0.454.ckpt')\n",
    "    # loaded_model = PETRegNet.load_from_checkpoint(saved_model_path)\n",
    "\n",
    "\n",
    "    loss = 0\n",
    "    loss_list = list()\n",
    "    time_list = list()\n",
    "    \n",
    "    y_list = list()\n",
    "    loss1 = []\n",
    "    \n",
    "\n",
    "    for test_data in test_loader:\n",
    "        x1 = test_data['ThreeD_Cloud_ref'].to(device)\n",
    "        x2 = test_data['ThreeD_Cloud_mov'].to(device)\n",
    "        x_t = test_data['pairs'].to(device)\n",
    "        time = test_data['delta_t']\n",
    "        y = test_data['T'].cpu().numpy()\n",
    "        # starter.record()\n",
    "        y_test = loaded_model(x1, x2,x_t).detach().cpu().numpy()\n",
    "        torch.cuda.synchronize() \n",
    "        loss = y-y_test\n",
    "        l = len(loss)\n",
    "        for j in range(l):\n",
    "            loss1=sum(np.square(loss[j]))/len(loss[j])\n",
    "            loss_list.append(loss1)\n",
    "            time_list.append(time.numpy()[j])\n",
    "            prediction_list.append(y_test[j])\n",
    "            y_list.append(y[j])\n",
    "        del test_data\n",
    "        idx= idx+1\n",
    "        \n",
    "    \n",
    "    df_results = pd.DataFrame()\n",
    "    y_list_all.append(y_list)\n",
    "    \n",
    "    df_results['Time'] = time_list\n",
    "    df_results['Loss'] = loss_list\n",
    "    df_results_all.append(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test = build_df_results(test_set, df_input_diff_all, prediction_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(data):\n",
    "    # data = list(data)\n",
    "    q1 = np.percentile(data, 25, interpolation='midpoint')\n",
    "    q3 = np.percentile(data, 75, interpolation='midpoint')\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n"
     ]
    }
   ],
   "source": [
    "tp = np.stack(df_results_test['model parameters'])\n",
    "tv = np.stack(df_results_test['vicra parameters'])\n",
    "# tp = np.stack(total_model)\n",
    "# tv = np.stack(total_vicra)\n",
    "tx = np.square(tv[:,0]-tp[:,0])\n",
    "ty = np.square(tv[:,1]-tp[:,1])\n",
    "tz = np.square(tv[:,2]-tp[:,2])\n",
    "\n",
    "rx = np.square(tv[:,3]-tp[:,3])\n",
    "ry = np.square(tv[:,4]-tp[:,4])\n",
    "rz = np.square(tv[:,5]-tp[:,5])\n",
    "\n",
    "mtx = np.mean(tx) \n",
    "mty = np.mean(ty) \n",
    "mtz = np.mean(tz)\n",
    "mt = (mtx + mty +mtz)/3  \n",
    "\n",
    "mrx = np.mean(rx) \n",
    "mry = np.mean(ry) \n",
    "mrz = np.mean(rz)\n",
    "mr = (mrx + mry + mrz)/3 \n",
    "\n",
    "st_list = []\n",
    "sr_list = []\n",
    "mmt_list = []\n",
    "mmr_list = []\n",
    "met_list=[]\n",
    "mer_list = []\n",
    "iqt_list=[]\n",
    "iqr_list=[]\n",
    "tmt_list=[]\n",
    "tst_list=[]\n",
    "tmet_list=[]\n",
    "tit_list=[]\n",
    "for i in range(20):\n",
    "    mmtx = np.mean(tx[i*1800:(i+1)*1800]) \n",
    "    mmty = np.mean(ty[i*1800:(i+1)*1800]) \n",
    "    mmtz = np.mean(tz[i*1800:(i+1)*1800])\n",
    "    mmt = (mmtx + mmty +mmtz)/3  \n",
    "    mmt_list.append(mmt)\n",
    "\n",
    "    mmrx = np.mean(rx[i*1800:(i+1)*1800]) \n",
    "    mmry = np.mean(ry[i*1800:(i+1)*1800]) \n",
    "    mmrz = np.mean(rz[i*1800:(i+1)*1800])\n",
    "    mmr = (mmrx + mmry + mmrz)/3 \n",
    "    mmr_list.append(mmr)\n",
    "\n",
    "\n",
    "    stx = np.std(tx[i*1800:(i+1)*1800]) \n",
    "    sty = np.std(ty[i*1800:(i+1)*1800]) \n",
    "    stz = np.std(tz[i*1800:(i+1)*1800])\n",
    "    st = (stx + sty +stz)/3  \n",
    "    st_list.append(st)\n",
    "\n",
    "    srx = np.std(rx[i*1800:(i+1)*1800]) \n",
    "    sry = np.std(ry[i*1800:(i+1)*1800]) \n",
    "    srz = np.std(rz[i*1800:(i+1)*1800])\n",
    "    sr = (srx + sry + srz)/3 \n",
    "    sr_list.append(sr)\n",
    "\n",
    "    metx = np.median(tx[i*1800:(i+1)*1800]) \n",
    "    mety = np.median(ty[i*1800:(i+1)*1800]) \n",
    "    metz = np.median(tz[i*1800:(i+1)*1800])\n",
    "    met = (metx + mety +metz)/3  \n",
    "    met_list.append(met)\n",
    "\n",
    "    merx = np.median(rx[i*1800:(i+1)*1800]) \n",
    "    mery = np.median(ry[i*1800:(i+1)*1800]) \n",
    "    merz = np.median(rz[i*1800:(i+1)*1800])\n",
    "    mer = (merx + mery +merz)/3  \n",
    "    mer_list.append(mer)\n",
    "\n",
    "    iqtx = iqr(tx[i*1800:(i+1)*1800]) \n",
    "    iqty = iqr(ty[i*1800:(i+1)*1800]) \n",
    "    iqtz = iqr(tz[i*1800:(i+1)*1800])\n",
    "    iqt = (iqtx + iqty +iqtz)/3  \n",
    "    iqt_list.append(iqt)\n",
    "\n",
    "    iqrx = iqr(rx[i*1800:(i+1)*1800]) \n",
    "    iqry = iqr(ry[i*1800:(i+1)*1800]) \n",
    "    iqrz = iqr(rz[i*1800:(i+1)*1800])\n",
    "    iqrr = (iqrx + iqry +iqrz)/3  \n",
    "    iqr_list.append(iqrr)\n",
    "    \n",
    "\n",
    "    tmx = np.mean(tx[i*1800:(i+1)*1800]+rx[i*1800:(i+1)*1800]) \n",
    "    tmy = np.mean(ty[i*1800:(i+1)*1800]+ry[i*1800:(i+1)*1800]) \n",
    "    tmz = np.mean(tz[i*1800:(i+1)*1800]+rz[i*1800:(i+1)*1800])\n",
    "    tmt = (tmx + tmy + tmz)/3  \n",
    "    tmt_list.append(tmt)\n",
    "    \n",
    "    tsx = np.std(tx[i*1800:(i+1)*1800]+rx[i*1800:(i+1)*1800]) \n",
    "    tsy = np.std(ty[i*1800:(i+1)*1800]+ry[i*1800:(i+1)*1800]) \n",
    "    tsz = np.std(tz[i*1800:(i+1)*1800]+rz[i*1800:(i+1)*1800])\n",
    "    tst = (tsx + tsy + tsz)/3  \n",
    "    tst_list.append(tst)\n",
    "\n",
    "    tmex = np.median(tx[i*1800:(i+1)*1800]+rx[i*1800:(i+1)*1800]) \n",
    "    tmey = np.median(ty[i*1800:(i+1)*1800]+ry[i*1800:(i+1)*1800]) \n",
    "    tmez = np.median(tz[i*1800:(i+1)*1800]+rz[i*1800:(i+1)*1800])\n",
    "    tmet = (tmex + tmey + tmez)/3  \n",
    "    tmet_list.append(tmet)\n",
    "    \n",
    "    tix = iqr(tx[i*1800:(i+1)*1800]+rx[i*1800:(i+1)*1800]) \n",
    "    tiy = iqr(ty[i*1800:(i+1)*1800]+ry[i*1800:(i+1)*1800]) \n",
    "    tiz = iqr(tz[i*1800:(i+1)*1800]+rz[i*1800:(i+1)*1800])\n",
    "    tit = (tix + tiy + tiz)/3  \n",
    "    tit_list.append(tit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(st_list, sr_list, mmt_list, mmr_list, met_list, mer_list, iqt_list, iqr_list, tmt_list, tst_list, tmet_list, tit_list):\n",
    "   mt,mr,st,sr,met,mer,it,ir,tm,ts,tme,tq =np.mean(mmt_list),np.mean(mmr_list),np.mean(st_list),np.mean(sr_list),np.mean(met_list), np.mean(mer_list),np.mean(iqt_list), np.mean(iqr_list),np.mean(tmt_list), np.mean(tst_list),np.mean(tmet_list), np.mean(tit_list)\n",
    "   return print(mt,st,met,it,mr,sr,mer,ir,tm,ts,tme,tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31551955807927 5.257200731721682 1.3593691154761676 2.6634826902675397 2.8423442085235258 3.3530695008560345 2.4823552508772457 4.140984513256411 5.157863766602796 7.759214748962546 4.180926186025017 6.569022051085443\n"
     ]
    }
   ],
   "source": [
    "cal(st_list, sr_list, mmt_list, mmr_list, met_list, mer_list, iqt_list, iqr_list, tmt_list, tst_list, tmet_list, tit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.734676492050678 12.438232688024492 5.455695533007011 10.373605568641922 4.785129250333939 5.275023775152056 3.7974137972547863 7.438375331102343 13.519805742384616 16.634821037950694 9.774156189144232 17.46659115807403\n"
     ]
    }
   ],
   "source": [
    "cal(st_list, sr_list, mmt_list, mmr_list, met_list, mer_list, iqt_list, iqr_list, tmt_list, tst_list, tmet_list, tit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.644660552343058 12.176103419218897 5.523794180872654 10.281121586120358 4.858054341860089 5.325544347075972 3.873080803321316 7.617762165329213 13.502714894203146 16.42654734896903 9.920247395572273 17.43873954391016\n"
     ]
    }
   ],
   "source": [
    "cal(st_list, sr_list, mmt_list, mmr_list, met_list, mer_list, iqt_list, iqr_list, tmt_list, tst_list, tmet_list, tit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_set:\n",
    "    save_synthetic_vicra('FDG', i, df_results_test, [3600,5399], 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-hmc_2301c",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
